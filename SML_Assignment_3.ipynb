{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "\n",
        "df = pd.read_csv('/content/data.csv')\n",
        "\n",
        "# Q1: Identify and count the number of duplicate rows in the dataset\n",
        "duplicate_rows = df[df.duplicated()]\n",
        "num_duplicates = duplicate_rows.shape[0]\n",
        "print(f\"Number of duplicate rows: {num_duplicates}\")\n",
        "\n",
        "# Q2: Remove the duplicate rows and display the size of the dataset before and after removal\n",
        "size_before_removal = df.shape\n",
        "df = df.drop_duplicates()\n",
        "size_after_removal = df.shape\n",
        "print(f\"Size before removing duplicates: {size_before_removal}\")\n",
        "print(f\"Size after removing duplicates: {size_after_removal}\")\n",
        "\n",
        "# Q3: Encode categorical columns using one-hot encoding\n",
        "df_encoded = pd.get_dummies(df, columns=['categorical_1', 'categorical_2'])\n",
        "\n",
        "# Q4: Normalize numerical columns using MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "df_encoded[['numerical_1', 'numerical_2']] = scaler.fit_transform(df_encoded[['numerical_1', 'numerical_2']])\n",
        "\n",
        "df_encoded.to_csv('/content/processed_data.csv', index=False)\n",
        "print(\"Processing complete. The processed data has been saved as 'processed_data.csv'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mAqHwJliWHUI",
        "outputId": "41efae33-10a5-4308-924d-43029d6e71aa"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of duplicate rows: 1\n",
            "Size before removing duplicates: (11, 5)\n",
            "Size after removing duplicates: (10, 5)\n",
            "Processing complete. The processed data has been saved as 'processed_data.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "df = pd.read_csv('/content/sentiment.csv')\n",
        "\n",
        "#Question 2\n",
        "label_encoder = LabelEncoder()\n",
        "df['source_encoded'] = label_encoder.fit_transform(df['source'])\n",
        "print(df[['source', 'source_encoded']].head())\n",
        "\n",
        "\n",
        "# Question 3\n",
        "df.fillna('null', inplace=True)\n",
        "print(df.head())\n",
        "\n",
        "\n",
        "# Question 4\n",
        "nltk.download('punkt')\n",
        "df['text_tokenized'] = df['text'].apply(word_tokenize)\n",
        "print(df[['text', 'text_tokenized']].head())\n",
        "\n",
        "\n",
        "# Quesiton 5\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "df['text_processed'] = df['text'].apply(lambda x: ' '.join([word.lower() for word in word_tokenize(x) if word.lower() not in stop_words]))\n",
        "print(df[['text', 'text_processed']].head())\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4qyEc0jY2cz",
        "outputId": "333a9189-0cdc-46db-d3ec-943c352320c8"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     source  source_encoded\n",
            "0   twitter               2\n",
            "1   twitter               2\n",
            "2  facebook               0\n",
            "3   twitter               2\n",
            "4  facebook               0\n",
            "   id                                               text        date  \\\n",
            "0   1  Over myself focus protect often impact at indu...  11-02-2023   \n",
            "1   2  Look agree contain she work size about majorit...  04-05-2021   \n",
            "2   3  Toward sport glass admit matter ever soldier a...  09-14-2023   \n",
            "3   4                                               null  07-21-2024   \n",
            "4   5                                               null  08-17-2022   \n",
            "\n",
            "  sentiment    source  source_encoded  \n",
            "0   neutral   twitter               2  \n",
            "1  positive   twitter               2  \n",
            "2  positive  facebook               0  \n",
            "3  positive   twitter               2  \n",
            "4   neutral  facebook               0  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                text  \\\n",
            "0  Over myself focus protect often impact at indu...   \n",
            "1  Look agree contain she work size about majorit...   \n",
            "2  Toward sport glass admit matter ever soldier a...   \n",
            "3                                               null   \n",
            "4                                               null   \n",
            "\n",
            "                                      text_tokenized  \n",
            "0  [Over, myself, focus, protect, often, impact, ...  \n",
            "1  [Look, agree, contain, she, work, size, about,...  \n",
            "2  [Toward, sport, glass, admit, matter, ever, so...  \n",
            "3                                             [null]  \n",
            "4                                             [null]  \n",
            "                                                text  \\\n",
            "0  Over myself focus protect often impact at indu...   \n",
            "1  Look agree contain she work size about majorit...   \n",
            "2  Toward sport glass admit matter ever soldier a...   \n",
            "3                                               null   \n",
            "4                                               null   \n",
            "\n",
            "                                      text_processed  \n",
            "0        focus protect often impact industry still .  \n",
            "1  look agree contain work size majority class pu...  \n",
            "2  toward sport glass admit matter ever soldier a...  \n",
            "3                                               null  \n",
            "4                                               null  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    }
  ]
}